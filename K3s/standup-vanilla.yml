- hosts: localhost
  become: no
  tasks:
    # Install checks
    - name: Make sure we have a virtual IP assigned
      fail:
        msg: To install k3s, you must pass an extra args for the virtual IP addresses when executing the play. E.g. "ansible-playbook K3s/standup-vanilla.yml -e 'vip=10.1.0.50 serviceRange=10.1.0.51-10.1.0.99' "
      when: ( vip is undefined ) or ( serviceRange is undefined )

    # Get needed resources
    - name: Create a remote-content directory
      file:
        path: ~/.HAB/remote_content/
        state: directory
    - name: Download k3s install script
      get_url:
        url: https://get.k3s.io
        dest: ~/.HAB/remote_content/k3s_install.sh
        mode: a+x
    - name: Get role based permission definitions for kube-vip
      get_url:
        url: https://kube-vip.io/manifests/rbac.yaml
        dest: ~/.HAB/remote_content/kube-vip-rbac.yaml
    - name: Get cloud-controler definitions for kube-vip
      get_url:
        url: https://raw.githubusercontent.com/kube-vip/kube-vip-cloud-provider/main/manifest/kube-vip-cloud-controller.yaml
        dest: ~/.HAB/remote_content/kube-vip-cloud-controller.yaml
    - name: Get cloud-controler definitions for kube-vip
      get_url:
        url: https://raw.githubusercontent.com/metallb/metallb/v0.13.7/config/manifests/metallb-native.yaml
        dest: ~/.HAB/remote_content/metallb-native.yaml
    - name: Create MetalLB config
      copy:
        dest: ~/.HAB/remote_content/metallb-config.yaml
        content: |-
          apiVersion: metallb.io/v1beta1
          kind: IPAddressPool
          metadata:
            name: first-pool
            namespace: metallb-system
          spec:
            addresses:
            - {{ serviceRange }}
          ---
          apiVersion: metallb.io/v1beta1
          kind: L2Advertisement
          metadata:
            name: l2-ads-for-global-ips
            namespace: metallb-system
          ---

- hosts: cluster
  become: yes
  tasks:
    # Copy needed resources
    - name: Copy install script to cluster
      copy:
        src: ~/.HAB/remote_content/k3s_install.sh
        dest: /tmp/k3s_install.sh
        mode: a+x

- hosts: leader
  gather_facts: false
  become: yes
  tasks:
    # Copy over needed resources
    - name: Copy remote resources to the cluster
      copy:
        src: ~/.HAB/remote_content/{{ item}}
        dest: /tmp/{{ item }}
      loop:
        - kube-vip-rbac.yaml
        - kube-vip-cloud-controller.yaml
        - metallb-native.yaml
        - metallb-config.yaml

    # Create or get server-token
    - name: Make sure we can read data from vault-server-token file
      become: no
      local_action:
        module: shell
        cmd: ANSIBLE_CONFIG=../ansible.cfg ansible-vault view ~/.HAB/vault-server-token
      register: serverTokenResults
      changed_when: false
      failed_when: false
      no_log: true
    - name: Create a new secret for server
      when: serverTokenResults.rc != 0
      shell: openssl rand -base64 63 | tr -d '\n'
      register: newSecret
      no_log: true
    - name: Write new vault definitions for vault-server-token to file
      when: serverTokenResults.rc != 0
      become: no
      local_action:
        module: shell
        cmd: 'echo "serverToken: {{ newSecret.stdout }}" | ANSIBLE_CONFIG=../ansible.cfg ansible-vault encrypt --output ~/.HAB/vault-server-token'
      no_log: true
    - name: Re-get serverToken
      set_fact:
        serverToken: "{{ lookup('file', '~/.HAB/vault-server-token').split(' ')[1] }}"
      no_log: true

    # Install k3s HA on leader host
    - name: Make sure k3s is not already running
      systemd:
        name: k3s
      register: results
    - name: If k3s is not already active...
      when: results.status.ActiveState == "inactive"
      block:
        - name: Execute k3s_install.sh on the leader host
          command: "/tmp/k3s_install.sh"
          environment:
            # consider adding: --flannel-backend wireguard-native and --secrets-encryption
            INSTALL_K3S_EXEC: "server --cluster-init --disable servicelb --disable traefik --log /tmp/k3s.log --tls-san {{ vip }} "
            K3S_TOKEN: "{{ serverToken }}"
            INSTALL_K3S_CHANNEL: latest
            K3S_KUBECONFIG_MODE: "644"
          changed_when: true
          async: 20
          poll: 5
          register: result
          retries: 100
          delay: 1
          until: result is not failed
          no_log: true

        # install kube-vip
        - name: Get latest release info for kube-vip
          become: no
          local_action:
            module: shell
            cmd: 'curl -sL https://api.github.com/repos/kube-vip/kube-vip/releases | jq -r ".[0].name"'
          register: release
        - name: Wait for config to populate so we can access the kubernetes instance
          wait_for:
            path: /etc/rancher/k3s/k3s.yaml
        - name: Install kube-vip RBAC
          shell: kubectl apply -f /tmp/kube-vip-rbac.yaml
          retries: 100
          delay: 5
          until: result is not failed
          register: result
        - name: Pull kube-vip image
          shell: "ctr image pull ghcr.io/kube-vip/kube-vip:{{ release.stdout }}"
          retries: 100
          delay: 5
          until: result is not failed
          register: result
        - name: Generate kube-vip manifests
          shell: ctr run
            --rm --net-host
            ghcr.io/kube-vip/kube-vip:{{ release.stdout }} vip /kube-vip manifest daemonset
            --address {{ vip }} --inCluster --taint --controlplane --services --arp --leaderElection > /tmp/kube-vip.yaml
        - name: Apply the generated manifest
          shell: kubectl apply -f /tmp/kube-vip.yaml
        - name: Apply the cloud controller
          shell: kubectl apply -f /tmp/kube-vip-cloud-controller.yaml

        # Copy node-token to vault
        - name: Copy Server node token to control computer local directory
          command: cat /var/lib/rancher/k3s/server/node-token
          register: nodeTokenRaw
          changed_when: false
        - name: Write new vault definitions for vault-node-token to file
          become: no
          local_action:
            module: shell
            cmd: 'echo "nodeToken: {{ nodeTokenRaw.stdout }}" | ANSIBLE_CONFIG=../ansible.cfg ansible-vault encrypt --output ~/.HAB/vault-node-token'
          no_log: true

- hosts: lieutenants
  gather_facts: false
  become: yes
  vars_files:
    - ~/.HAB/vault-node-token
  tasks:
    # Install k3s HA on master hosts
    - name: Make sure k3s is not already running
      systemd:
        name: k3s
      register: results
    - name: Execute k3s_install.sh for the other master hosts
      when: results.status.ActiveState == "inactive"
      command: "/tmp/k3s_install.sh"
      environment:
        INSTALL_K3S_EXEC: "server --server https://{{ vip }}:6443 --disable servicelb --disable traefik --log /tmp/k3s.log"
        K3S_TOKEN: "{{ nodeToken }}"
        INSTALL_K3S_CHANNEL: latest
        K3S_KUBECONFIG_MODE: "644"
      changed_when: true
      async: 20
      poll: 5
      register: result
      retries: 100
      delay: 1
      until: result is not failed
      no_log: true

- hosts: masters
  gather_facts: false
  become: yes
  tasks:
    - name: Make sure k3s is running on all master hosts
      systemd:
        name: k3s
        state: started

- hosts: workers
  gather_facts: false
  become: yes
  vars_files:
    - ~/.HAB/vault-node-token
  tasks:
    # Install k3s on and join worker hosts to the cluster
    - name: Make sure k3s-agent is not already running
      systemd:
        name: k3s-agent
      register: results
    - name: Join the cluster through the master host's DNS record by executing k3s_install.sh for worker hosts
      when: results.status.ActiveState == "inactive"
      command: "/tmp/k3s_install.sh --log /tmp/k3s.log"
      environment:
        K3S_TOKEN: "{{ nodeToken }}"
        K3S_URL: https://{{ vip }}:6443
        K3S_NODE_NAME: "{{ inventory_hostname }}"
        K3S_KUBECONFIG_MODE: "644"
        INSTALL_K3S_CHANNEL: latest
      changed_when: true
      async: 20
      poll: 5
      register: result
      retries: 100
      delay: 1
      until: result is not failed
      no_log: true
    - name: Make sure k3s-agent is running on all worker hosts
      systemd:
        name: k3s-agent
        state: started

- hosts: leader
  gather_facts: false
  become: yes
  tasks:
    # Label K3S worker hosts
    - name: Wait for worker hosts to be ready
      command: "kubectl get nodes {{ item }}"
      register: hosts
      until: '" Ready " in hosts.stdout'
      retries: 15
      delay: 5
      with_items: "{{ groups['workers'] }}"
      changed_when: false
    - name: Label worker hosts
      command: "kubectl label nodes {{ item }} kubernetes.io/role=worker"
      with_items: "{{ groups['workers'] }}"
      register: results
      failed_when:
        - results.rc != 0
        - '"already has a value (worker)" not in results.stderr'
      changed_when: false

    # Transfer the remote kube config to the local control machine
    - name: Copy k3s.yaml to config
      copy:
        remote_src: true
        src: /etc/rancher/k3s/k3s.yaml
        dest: /tmp/config
      register: copyChanged
    - name: Get the kube config file
      when: copyChanged.changed
      fetch:
        src: /tmp/config
        dest: ~/.kube/
        flat: yes
    - name: Edit the local config file to point to the master host
      when: copyChanged.changed
      become: no
      local_action:
        module: lineinfile
        path: "~/.kube/config"
        search_string: "127.0.0.1"
        line: "    server: https://{{ vip }}:6443"

    # Install MetalLB Load Balancer
    - name: Apply the metalLB load balancer
      shell: kubectl apply -f /tmp/metallb-native.yaml
    - name: Apply the metalLB config
      shell: kubectl apply -f /tmp/metallb-config.yaml
      retries: 10
      delay: 5
      register: result
      until: result.rc == 0
      failed_when: result.rc != 0
